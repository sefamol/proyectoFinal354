# -*- coding: utf-8 -*-
"""Proyecto 354 final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rXQ5XoK7c-bxJH9oWnQyLXUDN2BlScX6
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import warnings
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix
from sklearn.model_selection import KFold, cross_val_score

# %matplotlib inline
warnings.filterwarnings("ignore")

!pip install -q pydot

# Used for visualizing trees, but not strictly necessary
from sklearn.externals.six import StringIO  
from IPython.display import Image  
from sklearn.tree import export_graphviz
import pydotplus

"""## importando el dataset de entrenamiento y de prueba"""

from google.colab import files
uploaded = files.upload()

import io

"""Cantidad de pasajeros por sexo"""

train_df = pd.read_csv(io.BytesIO(uploaded['train.csv']))

from google.colab import files
uploaded = files.upload()

test_df = pd.read_csv(io.BytesIO(uploaded['test.csv']))

"""Descripción de los datos de las columnas del dataset. 


    survival: Survival (0 = No; 1 = Yes) (sobreviviente)
    pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) (categoria de pasajero)
    name: Name (nombre)
    sex: Sex (sexo)
    age: Age (edad)
    sibsp: Number of Siblings/Spouses Aboard (hermanas o conyugues abordo)
    parch: Number of Parents/Children Aboard (padres o hijos abordo)
    ticket: Ticket Number (núúmero de boleto)
    fare: Passenger Fare (boleta)
    cabin: Cabin (cabina)
    embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) (puerto de embarcación)


"""

train_df.head()

"""Descripción de los datos en función de la columna survived"""

train_df.Survived.describe()

"""Descripción de sobrevivientes por edad"""

train_df.loc[(train_df.Survived==0), 'Age'].hist(bins=20, alpha=.6, color='red', figsize=[15, 5])
train_df.loc[(train_df.Survived==1), 'Age'].hist(bins=20, alpha=.6, color='blue')

"""Descripción de sobrevivientes por sexo"""

train_df[['Sex', 'Survived']].groupby('Sex').agg(['mean', 'count'])

"""contabilizando nulos"""

train_df.isna().sum()

"""filtrando las muertes y los sobrevivientes por la clase del boleto de embarque"""

survived_class = pd.crosstab(index=train_df['Survived'],
                             columns = train_df['Pclass'],
                             margins = True)
survived_class.index = ["muertes", "sobrevivientes", "total_clase"]
survived_class.columns=["primera", "segunda", "tercera", "total_sobrev"]
survived_class

"""frecuencia relativa global"""

survived_class/survived_class.loc["total_clase", "total_sobrev"]

"""frecuencia relativa marginal"""

survived_class/survived_class.loc["total_clase"]

survived_class.div(survived_class["total_sobrev"], axis=0)

"""filtrando por sexo y tipo de boleto"""

surv_sex_class = pd.crosstab(index = train_df["Survived"],
                             columns = [train_df["Sex"], train_df["Age"]],
                             margins = True)
surv_sex_class

surv_sex_class/surv_sex_class.loc["All"]

"""## limpiando datos y preparandolos para crear el arbol de decisiones


 Creando valores booleanos para cada punto de embarque.
 Creando un valor booleano para is_male.
 Creando un valor booleano que determina si alguien tiene cabina.
 En caso de no existir edades reemplazamos NaN por el valor 100. 
"""

# Embarked
for k in train_df.Embarked.unique():
    if type(k)==str:
        train_df['emb_' + k] = (train_df.Embarked==k)*1
# Sex
train_df['is_male'] = (train_df.Sex=='male')*1

# Cabin
train_df.loc[:, 'has_cabin'] = 0
train_df.loc[train_df.Cabin.isna(), 'has_cabin'] = 1

# Age
train_df.loc[train_df.Age.isna(), 'Age'] = 100

print(list(train_df))
train_df.head()

"""limpiando y entrenando la lista"""

features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 
            'emb_S', 'emb_C', 'emb_Q', 'is_male', 'has_cabin']

valid = train_df[features].notna().all(axis=1)
print(len(train_df), sum(valid))

"""# Construyendo e implementando el arbol de decisiones
El algoritmo de aprendizaje supervisado realiza una predicción precisa y estable.

Una ventaja del bosque aleatorio es que se puede utilizar tanto para problemas de clasificación como de regresión. 

El algoritmo de bosque aleatorio aporta aleatoriedad adicional al modelo cuando hace crecer los árboles. En lugar de buscar la mejor característica mientras divide un nodo, busca la mejor característica entre un subconjunto aleatorio de características. Este proceso crea una amplia diversidad, que generalmente resulta en un mejor modelo. Por lo tanto, cuando está cultivando un árbol en un bosque aleatorio, solo se considera un subconjunto aleatorio de las características para dividir un nodo.
"""

dtree=DecisionTreeClassifier(
    criterion='entropy', 
    random_state=20181105, 
    #max_depth=5,
    min_samples_split=2, 
    #min_samples_leaf=1, 
    #max_features=None, 
    #max_leaf_nodes=None, 
)

dtree.fit(train_df[features], train_df['Survived'])

"""Visualización del arbol de decisiones"""

dot_data = StringIO()
export_graphviz(dtree, 
                out_file=dot_data,  
                filled=True, 
                rounded=True,
                feature_names=features,
                special_characters=True
               )
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
Image(graph.create_png())

"""calculando metricas a partir del rendimiento de la muestra"""

pred_survival = dtree.predict(train_df[features])

print(confusion_matrix(train_df.Survived, pred_survival), '\n')
print('Accuracy:   %0.3f' % accuracy_score(train_df.Survived, pred_survival))
print('Precision:  %0.3f' % precision_score(train_df.Survived, pred_survival))
print('Recall:     %0.3f' % recall_score(train_df.Survived, pred_survival))

"""modelo de regresión lineal para comparación"""

logreg = LogisticRegression(random_state=20181105, solver='lbfgs')
logreg.fit(train_df[features], train_df['Survived'])
pred_survival = logreg.predict(train_df[features])

print(confusion_matrix(train_df.Survived, pred_survival), '\n')
print('Accuracy:   %0.3f' % accuracy_score(train_df.Survived, pred_survival))
print('Precision:  %0.3f' % precision_score(train_df.Survived, pred_survival))
print('Recall:     %0.3f' % recall_score(train_df.Survived, pred_survival))

"""#Cross Validation

Usando la función KFold de sci-kit learn generamos siete splits para la validación cruzada. Así mostramos el saldo de la tasa de supervivencia entre los diferentes splits para tener una mejor idea de lo que está sucediendo.
     A continuación, entrenamos un modelo de árbol de decisión diferente en cada uno de los splits y realizamos un seguimiento de nuestro rendimiento.

"""

k_fold = KFold(n_splits=10, random_state=20181105)

# imprimiendo la cantidad de observaciones y el rango de sobrevivencia 
for train_indices, test_indices in k_fold.split(train_df[features]):
     print('Train: n=%i, s_rate=%0.2f | test: n=%i, s_rate=%0.2f ' % 
           (train_df.loc[train_indices, 'Survived'].count(), 
            train_df.loc[train_indices, 'Survived'].mean(), 
            train_df.loc[test_indices, 'Survived'].count(),
            train_df.loc[test_indices, 'Survived'].mean(),
           )
          )

"""Creando una función que se ajuste a nuestro modelo y devuelva métricas relevantes para facilitar el seguimiento del rendimiento de la validación cruzada."""

def get_cv_results(classifier):
    results = []
    for train, test in k_fold.split(train_df[features]):
        classifier.fit(train_df.loc[train, features], train_df.loc[train, 'Survived'])
        y_predicted = classifier.predict(train_df.loc[test, features])
        accuracy = accuracy_score(train_df.loc[test, 'Survived'], y_predicted)
        results.append(accuracy)
    
    return np.mean(results), np.std(results)

"""Utilizamos la media y la varianza para hallar la precisión de los diferentes valores de las muestras mínimas por split. """

hp_values = range(10,200, 10)
all_mu = []
all_sigma = []

for m in hp_values:

    dtree=DecisionTreeClassifier(
        criterion='entropy', 
        random_state=20180408, 
        min_samples_split=m, 
        #max_depth=m,
        #min_samples_leaf=m, 
        #max_features=m, 
        #max_leaf_nodes=m, 
    )

    mu, sigma = get_cv_results(dtree)
    all_mu.append(mu)
    all_sigma.append(sigma)
    
    print(m, mu, sigma)

"""Gráfica de la media y la varianza de la precisión para diferentes valores de las muestras por split"""

plt.figure(figsize=(14, 5))
plt.plot(hp_values, all_mu)
plt.ylabel('Cross Validation Accuracy')
plt.xlabel('Minimum Samples Per Leaf')

plt.figure(figsize=(14, 5))
plt.plot(hp_values, all_sigma)
plt.ylabel('Cross Validation Std Dev.')
plt.xlabel('Minimum Samples Per Leaf')

"""Podemos realizar una comparacióón con la regresióón"""

logreg = LogisticRegression(random_state=20181105, solver='lbfgs')
get_cv_results(logreg)

"""Realizando el conjundo de pruebas"""

dtree=DecisionTreeClassifier(
        criterion='entropy', 
        random_state=20181105, 
        min_samples_split=100, 
    )

# entrenando el modelo final para validar nuestros datos.
dtree.fit(train_df.loc[:, features], train_df.loc[:, 'Survived'])

# Embarkment 
for k in test_df.Embarked.unique():
    if type(k)==str:
        test_df['emb_' + k] = (test_df.Embarked==k)*1

# Sex boolean
test_df['is_male'] = (test_df.Sex=='male')*1

# Cabin boolean
test_df.loc[:, 'has_cabin'] = 0
test_df.loc[test_df.Cabin.isna(), 'has_cabin'] = 1

# Age 
test_df.loc[test_df.Age.isna(), 'Age'] = 100

# Fare 
test_df.loc[test_df.Fare.isna(), 'Fare'] = test_df.loc[test_df.Fare.notna(), 'Fare'].median()

print(list(test_df))
test_df.head()

"""utilizando nuestro modelo comparamos los resultados"""

# Calculando la probabilidad
test_probabilities = dtree.predict_proba(test_df[features])[:,1]
test_df['survival_likelihood'] = test_probabilities

readable_features =  ['Name', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 
                      'Ticket', 'Fare', 'Cabin', 'Embarked', 'survival_likelihood']

# encontrando el rango basado en la probabilidad
probability_rankings = np.argsort(test_probabilities)

test_df.loc[probability_rankings[-20:], readable_features]

test_df.loc[probability_rankings[:20], readable_features]

"""# Conclusiones
En base a los datos conocidos podemos deducir que existe una mayor probabilidad de sobrevivir si el pasajero es mujer y embarco con un boleto de primera clase. Tambien es posible deducir que si el boleto es de las clases superiores es más probable que sobreviva el pasajero.

"""